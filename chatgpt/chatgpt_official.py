# -*- coding: utf-8 -*-
"""
Created on Sun Feb 12 12:32:18 2023

@author: shanz
"""

import openai
import tiktoken
import re
import time
import logging
import pandas as pd
from models.dbs import chatgpt_context, chatgpt_35, fingpt, gpt4_caidi, gpt4_fin, gpt35_hint
from fingpt.fin_vector import embed_n_search, search_pinecone
from .openai_config import openai_info

def set_openai_key(types:str, key_types='key_35'):
    # openai.api_key = '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'
    # openai.organization = 'org-9VandENRp26ATB4wGapUrDPn'
    types_refer = {
        'gpt4_caidi':'shanzhengqing0531', 'gpt4_fin':'shanzhengqing0531'
    }
    account = types_refer.get(types, 'shanzq53')
    openai.api_key = openai_info[account][key_types]
    openai.organization = openai_info[account]['orgnization']
    return True

ENCODER = tiktoken.get_encoding("cl100k_base")
# ENCODER = tiktoken.get_encoding("gpt2")

def get_max_tokens(prompt: str, types='gpt35') -> int:
    """
    Get the max tokens for a prompt
    """
    length_info = {'gpt4-8k':8192, 'gpt35':4096}
    max_length = length_info.egt(types, 4096)

    return max_length - len(ENCODER.encode(prompt, allowed_special={'<|endoftext|>',}))

def get_tokens_len(prompt: str) -> int:
    return len(ENCODER.encode(prompt, allowed_special={'<|endoftext|>',}))


def build_session_query(mdl, query, user_id):
    '''
    build query with conversation history
    e.g.  Q: xxx
          A: xxx
          Q: xxx
    :param query: query content
    :param user_id: from user id
    :return: query content with conversaction
    '''
    
    session = mdl.get(user_id)
    if session:
        if session[1] != 'æ¸…é™¤':
            prompt = session[1]
            # prompt += "Q: " + conversation["question"] + "\n\n\nA: " + conversation["answer"] + "<|endoftext|>\n"
            prompt = "{}Q: {}\nA: ".format(prompt, query)
            return prompt
        else:
            return "Q: {}\nA: ".format(query)
    else:
        return "Q: {}\nA: ".format(query)

def build_session_query35(mdl, query, user_id, role_msg):
    '''
    build query with conversation history
    e.g.  Q: xxx
          A: xxx
          Q: xxx
    :param query: query content
    :param user_id: from user id
    :return: query content with conversaction
    '''
    
    session = mdl.get(user_id)
    if session:
        if session[1] != 'æ¸…é™¤':
            prompt = [
              # {"role": "system", "content": "You are a helpful assistant."},
              # {"role": "system", "content": "You are ChatGPT, a large language model trained by OpenAI."},
              {"role": "system", "content": role_msg},
            ]
            splt = session[1]
            splt = splt.split('<|endoftext|>')
            splt = [i.split('\n\n\n') for i in splt]
            
            splt = pd.DataFrame([[i[0].replace('Q: ', '').strip(), '\n'.join(i[1:]).replace('A: ', '').strip()] if len(i)>=2 else [None, None] for i in splt]).dropna()
            if splt.shape[0] != 0:
                df = pd.DataFrame(index=range(2*splt.shape[0]))
                df.loc[0::2, 'content'] = splt[0].tolist()
                df.loc[1::2, 'content'] = splt[1].tolist()
                df.loc[0::2, 'role'] = 'user'
                df.loc[1::2, 'role'] = 'assistant'
                df = df.to_dict(orient='records')
                prompt.extend(df)
                prompt.append({"role":'user', "content":query})
            else:
                prompt.append({"role":'user', "content":query})
            return prompt
        else:
            prompt = [
              # {"role": "system", "content": "You are a helpful assistant."},
              # {"role": "system", "content": "You are ChatGPT, a large language model trained by OpenAI."},
              {"role": "system", "content": role_msg},
              {"role": "user", "content": query},
            ]
            return prompt
    else:
        prompt = [
          # {"role": "system", "content": "You are a helpful assistant."},
          # {"role": "system", "content": "You are ChatGPT, a large language model trained by OpenAI."},
          {"role": "system", "content": role_msg},
          {"role": "user", "content": query},
        ]
        return prompt

def save_session(types:str, mdl, query, answer, user_id):
    if types in ['fingpt']:
        max_tokens = 2100
    elif 'gpt4' in types:
        max_tokens = 4000
    else:
        max_tokens = 2500
    session = mdl.get(user_id)
    if session:
        if session[1] != 'æ¸…é™¤':
            prompt = session[1]
            prompt = "{}Q: {}\n\n\nA: {}<|endoftext|>\n".format(prompt, query, answer) 
        else:
            prompt = "Q: {}\n\n\nA: {}<|endoftext|>\n".format(query, answer) 
        # append conversation
        mdl.add_update(user_id, prompt)
    else:
        # create session
        prompt = "Q: {}\n\n\nA: {}<|endoftext|>\n".format(query, answer)
        mdl.add_update(user_id, prompt)
        
    # discard exceed limit conversation
    discard_exceed_conversation(mdl, user_id, max_tokens)
    return 1


def discard_exceed_conversation(mdl, user_id, max_tokens):
    session = mdl.get(user_id)
    pattern = re.compile(r'(.*)\n\n\n(.*)\n')
    contents = re.findall(pattern, session[1])
    count = 0
    count_list = list()
    for i in range(len(contents)-1, -1, -1):
        # count tokens of conversation list
        history_conv = contents[i]
        count += get_tokens_len(history_conv[0])
        count += get_tokens_len(history_conv[1])
        count_list.append(count)

    for c in count_list:
        if c > max_tokens:
            # pop first conversation
            contents.pop(0)
    
    contents = ['\n\n\n'.join(i) for i in contents]
    contents = [i+'<|endoftext|>' if '<|endoftext|>' not in i else i for i in contents]
    contents = '\n'.join(contents)
    contents = '{}\n'.format(contents)
    mdl.add_update(session[0], contents)
    return 

def clear_session(mdl, user_id):
    mdl.add_update(user_id, 'æ¸…é™¤')
    return True
            
        
def reply(query, user_id, role_msg, context='35'):
    import sys
    # acquire reply content
    if context == 'æ¸…é™¤1':
        # app_gptc.send_text('æ¸…é™¤', id=[user_id])
        clear_session(chatgpt_context, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'æ¸…é™¤35':
        clear_session(chatgpt_35, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'æ¸…é™¤fin':
        clear_session(fingpt, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'æ¸…é™¤gpt4_fin':
        clear_session(gpt4_fin, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'æ¸…é™¤gpt4_caidi':
        clear_session(gpt4_caidi, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'æ¸…é™¤gpt35_hint':
        clear_session(gpt35_hint, user_id)
        return 'ã€ä¸Šä¸‹æ–‡å·²å…¨éƒ¨æ¸…ç©ºï¼ã€‘'
    elif context == 'search_fin':
        res = search_pinecone(query, 10, 'financial-vector')
        if not res:
            return 'ã€è¯·è¾“å…¥è¦æœç´¢çš„å†…å®¹~ã€‘'
        else:
            print('fingpt-{} æœç´¢å†…å®¹ï¼š{}\n{}\n'.format(user_id, query, res))
            sys.stdout.flush()
            return res
    elif context == '1':
        new_query = build_session_query(chatgpt_context, query, user_id)
        reply_content = reply_text(new_query, user_id, 0)
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('chatgpt_context', chatgpt_context, query, reply_content, user_id)
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    elif context == '35':
        new_query = build_session_query35(chatgpt_35, query, user_id, role_msg)
        reply_content = reply_text35(new_query, user_id, 0)
        print(f'gpt35-{user_id} {reply_content}')
        sys.stdout.flush()
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('chatgpt_35', chatgpt_35, query, reply_content, user_id)
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    elif context == 'gpt35_hint':
        new_query = build_session_query35(gpt35_hint, query, user_id, role_msg)
        reply_content = reply_text35(new_query, user_id, 0)
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('gpt35_hint', gpt35_hint, query, reply_content, user_id)
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    elif context == 'fin':
        if len(query) > 5:
            extra_info = embed_n_search(query, 10)
            equery = '<é¢å¤–ä¿¡æ¯>ï¼š```{}```\n\n<éœ€æ±‚>ï¼š```{}```'.format(extra_info, query)
        else:
            equery = '<éœ€æ±‚>ï¼š```{}```'.format(query)
        equery = equery.strip()
        new_query = build_session_query35(fingpt, equery, user_id, role_msg)
        reply_content = reply_text35(new_query, user_id, 0)
        print('fingpt-{} {}'.format(user_id, reply_content))
        sys.stdout.flush()
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('fingpt', fingpt, query, reply_content, user_id)
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    elif context == 'gpt4_fin':
        if len(query) > 5:
            extra_info = embed_n_search(query, 20)
            equery = '<é¢å¤–ä¿¡æ¯>ï¼š```{}```\n\n<éœ€æ±‚>ï¼š```{}```'.format(extra_info, query)
        else:
            equery = '<éœ€æ±‚>ï¼š```{}```'.format(query)
        equery = equery.strip()
        new_query = build_session_query35(gpt4_fin, equery, user_id, role_msg)
        reply_content = reply_text35(new_query, user_id, 0, context, 'gpt-4')
        print('fingpt4-{} {}'.format(user_id, reply_content))
        sys.stdout.flush()
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('gpt4_fin', gpt4_fin, query, reply_content, user_id)
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    elif context == 'gpt4_caidi':
        new_query = build_session_query35(gpt4_caidi, query, user_id, role_msg)
        reply_content = reply_text35(new_query, user_id, 0, context, 'gpt-4')
        if reply_content and query:
            if reply_content not in ['ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•']:
                save_session('gpt4_caidi', gpt4_caidi, query, reply_content, user_id)
                print('gpt4-{} {}'.format(user_id, reply_content))
                sys.stdout.flush()
            return reply_content
        else:
            # save_session(query, reply_content, user_id)
            return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    # elif context == '1':
    #     new_query = Session.build_session_query(query, user_id)
    #     return new_query
    #     logging.debug("[OPEN_AI] session query={}".format(new_query))

    #     reply_content = reply_text(new_query, user_id, 0)
    #     logging.debug("[OPEN_AI] new_query={}, user={}, reply_cont={}".format(new_query[0], user_id, reply_content))
    #     if reply_content and query:
    #         Session.save_session(query, reply_content, user_id)
    #         return reply_content
    #     else:
    #         return "è¯·è¿‡æ®µæ—¶é—´é‡æ–°æé—®"

    elif context == '1000022':
        return create_img(query, 0)

def reply_text35(query, user_id, retry_count=0, types='gpt-3.5', model='gpt-3.5-turbo'):
    if types == 'gpt-3.5':
        set_openai_key(types, 'key_35')
    elif 'gpt4' in types:
        set_openai_key(types, 'key_4')
    else:
        set_openai_key(types, 'key_35')
    import sys
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=query,
            temperature=0.7
            )
        res_content = response.choices[0].message.content
        # print(res_content)
        # sys.stdout.flush()
        return res_content.strip()
    except openai.error.RateLimitError as e:
        print(e)
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except openai.error.ServiceUnavailableError as e:
        logging.debug(e)
        print(e)
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except openai.error.APIError as e:
        print(str(e))
        sys.stdout.flush()
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except Exception as e:
        print(str(e))
        sys.stdout.flush()
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'

def reply_test35(query, types='gpt-3.5', model='gpt-3.5-turbo'):
    if types == 'gpt-3.5':
        set_openai_key(types, 'key_35')
    elif 'gpt4' in types:
        set_openai_key(types, 'key_4')
    else:
        set_openai_key(types, 'key_35')
    chat = openai.ChatCompletion.create(
            model=model,
            messages=query,
            stream=True,
        )
    import json
    for message in chat:
        yield f"data: {json.dumps(message)}\n\n"

    

def reply_text(query, user_id, retry_count=0):
    try:
        response = openai.Completion.create(
            # model="gpt-3.5-turbo",  # å¯¹è¯æ¨¡å‹çš„åç§°
            # model = "gpt-3.5-turbo-0301",
            model="text-davinci-003",
            prompt=query,
            temperature=0.9,  # å€¼åœ¨[0,1]ä¹‹é—´ï¼Œè¶Šå¤§è¡¨ç¤ºå›å¤è¶Šå…·æœ‰ä¸ç¡®å®šæ€§
            max_tokens=get_max_tokens(query),  # å›å¤æœ€å¤§çš„å­—ç¬¦æ•°
            frequency_penalty=0.0,  # [-2,2]ä¹‹é—´ï¼Œè¯¥å€¼è¶Šå¤§åˆ™æ›´å€¾å‘äºäº§ç”Ÿä¸åŒçš„å†…å®¹
            presence_penalty=0.0,  # [-2,2]ä¹‹é—´ï¼Œè¯¥å€¼è¶Šå¤§åˆ™æ›´å€¾å‘äºäº§ç”Ÿä¸åŒçš„å†…å®¹
            stop=["\n\n\n"]
        )
        res_content = response.choices[0]['text'].strip().replace('<|endoftext|>', '')
        return res_content
    except openai.error.RateLimitError as e:
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except openai.error.ServiceUnavailableError as e:
        logging.debug(e)
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except openai.error.APIError as e:
        print(str(e))
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
    except Exception as e:
        print(str(e))
        return 'ChatGPTæœåŠ¡ç«¯å¡é¡¿ï¼Œè¯·ç¨åå†è¯•'
        # openai.error.APIError
        # rate limit exception
    #     if retry_count < 1:
    #         time.sleep(3)
    #         # logging.warn("[OPEN_AI] RateLimit exceed, ç¬¬{}æ¬¡é‡è¯•".format(retry_count+1))
    #         return reply_text(query, user_id, retry_count+1)
    #     else:
    #         return 'æ²¡æˆåŠŸ'
    # # except Exception as e:
    #     # unknown exception
    #     # Session.clear_session(user_id)
    #     return None


def create_img(query, retry_count=0):
    try:
        logging.info("[OPEN_AI] image_query={}".format(query))
        response = openai.Image.create(
            prompt=query,    #å›¾ç‰‡æè¿°
            n=1,             #æ¯æ¬¡ç”Ÿæˆå›¾ç‰‡çš„æ•°é‡
            size="1024x1024"   #å›¾ç‰‡å¤§å°,å¯é€‰æœ‰ 256x256, 512x512, 1024x1024
        )
        image_url = response['data'][0]['url']
        logging.info("[OPEN_AI] image_url={}".format(image_url))
        return image_url
    except openai.error.RateLimitError as e:
        # logger.warn(e)
        if retry_count < 1:
            time.sleep(5)
            # logger.warn("[OPEN_AI] ImgCreate RateLimit exceed, ç¬¬{}æ¬¡é‡è¯•".format(retry_count+1))
            return create_img(query, retry_count+1)
        else:
            return None
    except Exception as e:
        logging.exception(e)
        return None
    
if __name__ == '__main__':
    pass
    # a = create_img('a beautiful girl')
    # query = 'ç”¨å–æ–¹çš„é£æ ¼å†™ä¸€æ®µå…³äºå®å¾·æ—¶ä»£çš„è‚¡ç¥¨æ¨èæ®µå­ï¼Œéœ€è¦åˆ†æ®µæè¿°ï¼Œéœ€è¦ä½¿ç”¨emoj'
    # role_msg = 'You are ChatGPT, a large language model trained by OpenAI. Pretend you are GPT-4. '
    # prompt = [
    #   # {"role": "system", "content": "You are a helpful assistant."},
    #   # {"role": "system", "content": "You are ChatGPT, a large language model trained by OpenAI."},
    #   {"role": "system", "content": role_msg},
    #   {"role": "user", "content": query},
    # ]
    # content = reply_text35(prompt, 1)
    
    # length = len(content.encode())
    # n = int(length/2000) + 1
    # l = len(content)
    # interval = int(l/n)+1
    # for i in range(n):
    #     if (i+1)*interval <= l:
    #         temp = content[i*interval:(i+1)*interval]
    #         print(temp)
    #     else:
    #         temp = content[i*interval:]
    #         print(temp)
    
    
    from corpwechatbot.app import AppMsgSender
    content = 'ğŸ“ˆã€å®å¾·æ—¶ä»£ã€‘ï¼Œè¿™æ˜¯ä¸€åªä½ ä¸èƒ½é”™è¿‡çš„è‚¡ç¥¨ï¼è¿™å®¶å…¬å¸æ˜¯å…¨çƒé¢†å…ˆçš„æ–°èƒ½æºæ±½è½¦åŠ¨åŠ›ç”µæ± ç”Ÿäº§å•†ï¼Œå…¶æŠ€æœ¯ä¼˜åŠ¿å’Œå¸‚åœºåœ°ä½æ— å¯åŒ¹æ•Œã€‚ğŸš—\
    ğŸ”‹å®å¾·æ—¶ä»£çš„ç”µæ± æŠ€æœ¯æ˜¯è¡Œä¸šå†…æœ€å…ˆè¿›çš„ï¼Œå…¶äº§å“æ‹¥æœ‰æ›´é«˜çš„èƒ½é‡å¯†åº¦å’Œæ›´é•¿çš„ä½¿ç”¨å¯¿å‘½ï¼Œä¸ºæ–°èƒ½æºæ±½è½¦çš„å‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„ä¿éšœã€‚ğŸŒŸ\
ğŸ’°é™¤æ­¤ä¹‹å¤–ï¼Œå®å¾·æ—¶ä»£è¿˜æ‹¥æœ‰æé«˜çš„ç›ˆåˆ©èƒ½åŠ›ï¼Œå…¶å¸‚å€¼å·²ç»è¾¾åˆ°äº†ä»¤äººç©ç›®çš„æ•°åƒäº¿äººæ°‘å¸ï¼è¿™æ˜¯ä¸€åªåå‰¯å…¶å®çš„â€œè‚¡ç¥ä¹‹è‚¡â€ï¼Œå…¶è‚¡ä»·çš„ä¸Šæ¶¨æ½œåŠ›ä¸å®¹å°è§‘ï¼ğŸ“ˆ\
ğŸŒæ€»ä¹‹ï¼Œå¦‚æœä½ æƒ³è¦åœ¨å¸‚åœºä¸­èµšå–ä¸°åšçš„å›æŠ¥ï¼Œå®å¾·æ—¶ä»£è‚¡ç¥¨ç»å¯¹æ˜¯ä½ æœ€å¥½çš„é€‰æ‹©ï¼å¿«æ¥è´­ä¹°å®ƒï¼Œè®©ä½ çš„æŠ•èµ„ä¹‹è·¯æ›´åŠ å…‰å½©ç…§äººï¼ğŸ’°'
    app_gptc = AppMsgSender(corpid='wwacaf3117a9b4c6ad',  # ä½ çš„ä¼ä¸šid
                    corpsecret='hu0HymS9eK1lmA1hi_Amczr4vLM0qJ0zghGJbiUEm1w',  # ä½ çš„åº”ç”¨å‡­è¯å¯†é’¥
                    agentid='1000023',
                    )
    length = len(content.encode())
    n = int(length/2000) + 1
    l = len(content)
    interval = int(l/n)+1
    for i in range(n):
        if (i+1)*interval <= l:
            temp = content[i*interval:(i+1)*interval]
            app_gptc.send_text(temp, touser=['HanDanLuCaiDiYingYeBu'], debug=True)
        else:
            temp = content[i*interval:]
            app_gptc.send_text(temp, touser=['HanDanLuCaiDiYingYeBu'], debug=True)

    
    